# -*- coding: utf-8 -*-
"""
Created on Thu Oct 29 10:26:47 2020

@author: remi
"""

import numpy as np
import scipy.sparse as sparse
import pandas as pd
import json
import time
from sklearn import linear_model
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
import copy
from sklearn.model_selection import GridSearchCV
import networkx as nx
import community


t1=time.time()
def get_number_link(id, df_know_infos, df_train_edges, page_types, link_depth):
    temp_df=df_train_edges[df_train_edges['id_2'].isin([id])]['id_1'].append(df_train_edges[df_train_edges['id_1'].isin([id])]['id_2'])
    for iter in range(link_depth):
        temp_df=df_train_edges[df_train_edges['id_2'].isin(temp_df)]['id_1'].append(df_train_edges[df_train_edges['id_1'].isin(temp_df)]['id_2'])
    know_links=df_know_infos[df_know_infos['id'].isin(temp_df)]
    dict={}
    nb_link=know_links.shape[0]
    dict.update({'nb_link': nb_link})
    if nb_link>0:
        for page_type in page_types:
            dict.update({page_type :know_links[know_links['page_type']==page_type]['id'].count()/nb_link})
    else:
        for page_type in page_types:
            dict.update({page_type : 0})
    
    return dict

def get_features_ratios(id,features):
    my_dict={}
    my_feature=features.get(str(id))
    my_dict.update({'mean' : np.mean(my_feature)})
    my_dict.update({'std' : np.std(my_feature)})
    my_dict.update({'sharpe' :  np.mean(my_feature)/np.std(my_feature)})
    my_dict.update({'nb_obs' :  len(my_feature)})
    my_dict.update({'sum' :  sum(my_feature)})
    return my_dict

def create_X(df_target,page_types, df_know_infos, df_edges, features,X_ids, partition_dict ):
    
    nb_page_type=len(page_types)
    nb_ids=len(X_ids)
    X=np.zeros((nb_ids, (1+nb_page_type)*4+6))
    for iter in range(nb_ids):
        id=X_ids[iter]
        direct_link=get_number_link(id, df_know_infos, df_edges, page_types, 0)
        second_link=get_number_link(id, df_know_infos, df_edges, page_types, 1)
        third_link=get_number_link(id, df_know_infos, df_edges, page_types, 2)
        fourth_link=get_number_link(id, df_know_infos, df_edges, page_types, 3)
        id_features=get_features_ratios(id, features)
        ids_community=partition_dict.get(id)
        row_values=[ids_community]+list(direct_link.values())+list(second_link.values())+list(third_link.values())+list(fourth_link.values())+list(id_features.values())
        X[iter,:]=row_values
    return X

def find_custom_model(Y, X, CVD_parameters,parameters_lasso, page_types):
    new_ratios=np.zeros((len(Y),len(page_types)))
    X_centered=copy.deepcopy(X)
    X_centered=X_centered-X_centered.mean(axis=0)
    model_dict={}
    count=0
    for page_type in page_types:
        new_Y=np.array(list(map(lambda x: 1 if x==page_type else 0, Y)))
        new_Y_mean=np.mean(new_Y)
        new_Y=new_Y-new_Y_mean
        best_lasso=GridSearchCV(linear_model.Lasso(max_iter=20000, tol=0.01, fit_intercept=False, selection='random'), parameters_lasso, n_jobs=4, verbose=0).fit(X_centered, new_Y).best_estimator_
        #best_lasso=linear_model.Lasso(max_iter=20000, tol=0.01, selection='random').fit(X_centered, new_Y)
        new_ratios[:,count]=best_lasso.predict(X_centered)+new_Y_mean
        model_dict.update({page_type: [best_lasso, new_Y_mean]})
        count+=1;
    clf = GridSearchCV(DecisionTreeClassifier(), CVD_parameters, n_jobs=4, verbose=0).fit(X=new_ratios, y=Y)
    tree_model = clf.best_estimator_
    predictions=tree_model.predict(new_ratios)
    print (clf.best_score_, clf.best_params_) 
    model_dict.update({'decision_tree': tree_model})
    return model_dict, predictions   
    
def use_custom_model_for_prediction(X, model_dict, page_types):
    new_ratios=np.zeros((np.shape(X)[0],len(page_types)))
    X_centered=copy.deepcopy(X)
    X_centered=X_centered-X_centered.mean(axis=0)
    count=0
    for page_type in page_types:
        new_Y_mean=model_dict.get(page_type)[1]
        new_ratios[:,count]=model_dict.get(page_type)[0].predict(X_centered)+new_Y_mean
        count+=1;
    predictions=model_dict.get('decision_tree').predict(new_ratios)
    return predictions

parameters = {'max_depth':range(1,23), 'criterion' :['gini', 'entropy'],'splitter': ['best','random'], 'min_samples_split' : range(3,6),'min_samples_leaf': range(1,5), 'min_impurity_decrease': [i/10 for i in range(40)] }
parameters_lasso={'alpha':[i/5 for i in range(10)]}
CVD_parameters = {'max_depth':range(1,8), 'criterion' :['gini', 'entropy'],'splitter': ['best','random'], 'min_samples_split' : range(3,6),'min_samples_leaf': range(1,5), 'min_impurity_decrease': [i/10 for i in range(40)] }

df_edges=pd.read_csv("musae_facebook_edges.csv")
df_target=pd.read_csv("musae_facebook_target.csv")
features=json.load(open("musae_facebook_features.json"))
nb_pages=df_target.shape[0] 
page_types=df_target['page_type'].unique()

df_know_infos=df_target[df_target['id']<nb_pages*0.05][['id','page_type']]

df_train_edges=df_edges[df_edges['id_1']<nb_pages*0.7]
df_train_target=df_target[df_target['id'].map(lambda x: True if x<0.6*nb_pages and x>0.05*nb_pages else False)==True]

df_test_edges=df_edges
df_test_target=df_target[df_target['id'].map(lambda x: True if x>0.6*nb_pages else False)==True]

Y_train=df_train_target['page_type'].values
Y_test=df_test_target['page_type'].values

##### Create a graph of the values and get the partition through Louvain  ######
X_ids=df_target['id'].values
G = nx.Graph()
G.add_nodes_from(X_ids)
ids_1 = df_edges['id_1'].values
ids_2 = df_edges['id_2'].values
for iter in range(len(ids_1)):
    G.add_edge(ids_1[iter],ids_2[iter])
partition_dict = community.best_partition(G)

nb_iter=4
initial_known_informations=copy.deepcopy(df_know_infos)
my_models=[]
for iter in range(nb_iter):
    print("starting iteration "+str(iter)+" on training part")
    X_ids_train=df_train_target['id'].values
    

    X_train=create_X(df_train_target, page_types, df_know_infos, df_edges,features,X_ids_train, partition_dict)

    model_dict, new_informations=find_custom_model(Y_train, X_train, CVD_parameters,parameters_lasso, page_types)

    df_know_infos=initial_known_informations.append(pd.DataFrame({'id':X_ids_train, 'page_type':new_informations}))
    my_models.append(model_dict)
    
    
df_know_infos=copy.deepcopy(initial_known_informations)
print("starting on test set")
for model_dict in my_models:
    X_ids_test=df_test_target['id'].values
    
    X_test=create_X(df_test_target, page_types, df_know_infos, df_edges,features,X_ids_test, partition_dict)
    
    new_informations=use_custom_model_for_prediction(X_test, model_dict, page_types)
    df_know_infos=initial_known_informations.append(pd.DataFrame({'id':X_ids_test, 'page_type':new_informations}))
    
    print("current classification report:")
    classification_report=metrics.classification_report(Y_test,new_informations)
    print(classification_report)


print("elapsed time "+str(time.time()-t1)+" seconds")